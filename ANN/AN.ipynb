{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPn3iqjnZ0nv98VWv+fmMxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khoaht312/computational-intelligence/blob/main/ANN/AN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Artificial Neuron**\n"
      ],
      "metadata": {
        "id": "l7Odd7YJJcjn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I. Definition**\n",
        "- An $AN$, implements a nonlinear mapping from $ℝ^I$ usually to $[ 0,1 ] \\text{ or } [-1,1]$, depending on the activation function used. That is,\n",
        "\n",
        "$f_{AN}:ℝ^I \\to [0,1] \\text{ or }f_{AN}:ℝ^I \\to [-1,1]$ ($I$ is the number of input signals to the $AN$).\n",
        "- An $AN$ receives a vector of $I$ input signals,\n",
        "\n",
        "$\t\\mathbf{z} = (z_1,z_2,...,z_I)$\n",
        "\n",
        "- To each input signal, $z_i$ is associated a weight, $v_i$, to strengthen or deplete the input signal.\n",
        "\n",
        "- The $AN$ computes the net input signal, and uses an activation function $f_{AN}$ to compute the output signal, $o$, given the net input. The strength of the output signal is further influenced by a threshold value $θ$, also refered to as the $bias$.\n",
        "```\n",
        "z1------v1---------\n",
        "                    |\n",
        "                    v\n",
        "z2------v2------> f(net-θ) ------>O\n",
        "                    |\n",
        "                    ^\n",
        "zI------vI---------\n",
        "```"
      ],
      "metadata": {
        "id": "W5c2sdrtJicX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Calculating the Net Input Signal**\n",
        "\n",
        "- The net input signal to an $AN$ is usually computed as the weighted sum of all input signals, $net = ∑_{i=1}^Iz_iv_i$.\n",
        "\n",
        "- Artificial neurons that compuyr the net input signal as the weighted sum of input signals are refered to as $\\text{ summation units (SU)}$. An alternative to compute the net input signal is to use $\\text{ product units (PU)}$, where $net=∏_{i=1}^Iz_i^{v_i}$,\n",
        "\n"
      ],
      "metadata": {
        "id": "wc_146J7NAo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Activation Functions**\n",
        "- The function $f_{AN}$ receives the net input signal and bias, and determines the output (or firing strength) of the neuron. This function is refered to as the $\\text{activation function }$.\n",
        "\n",
        "1. **Linear Function**:\n",
        "- $f_{AN}(net - θ) = λ(net - θ)$, where $λ$ is the slope of the function. The linear function produces a linearly modulated output, where $λ$ is a constant.\n",
        "\n",
        "2. **Step Function**:\n",
        "- $f_{AN}(net-θ) =\n",
        "\\begin{cases}\n",
        "γ_1 & \\text{if net ≥ θ}\\\\\n",
        "γ_2 & \\text{if net < θ}\\\\\n",
        "\\end{cases}$\n",
        "\n",
        "- The step function produces one of two scalar output values, depending on the value of the threshold $θ$. Usually, a binary output is produced for which $γ_1 = 1$ and $γ_2=0$; a bipolar output is also sometimes used where $γ_1 = 1$ and $y_2 = -1$.\n",
        "\n",
        "3. **Ramp function**:\n",
        "- $f_{AN}(net - θ)=\n",
        "\\begin{cases}\n",
        "γ & \\text{ if net - θ ≥ ϵ}\\\\\n",
        "net - θ & \\text{ if - ϵ < net - θ < ϵ}\\\\\n",
        "-γ  &  \\text{if net - θ ≤ - ϵ}\n",
        "\\end{cases}$\n",
        "\n",
        "- The ramp function is a combination of the linear and step functions.\n",
        "\n",
        "4. **Sigmoid function**:\n",
        "- $f_{AN}(net - θ) = \\frac{1}{1+e^{-λ(net - θ)}}$.\n",
        "- The sigmoid function is a continuous version of the ramp function, with $f_{AN}(net) \\in (0,1)$. The parameter $λ$ controls the steepness of the function. Usually, $λ = 1$.\n"
      ],
      "metadata": {
        "id": "YGvkzaU_QkWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IV. Artificial Neuron Learning**\n",
        "\n",
        "- The $AN$ learns the best values for the $v_i$ and $θ$ from the given data. Learning consists of adjusting weight and threshold values until a certain criterion is (are) satisfied.\n",
        "\n",
        "- **Supervised learning**: where the neuron (NN) is provided with a data set consisting of input vectors and a target (desired output) associated with each input vector. This data set is refered to as the training set. The aim of supervised training is then to adjust the weight values such that the error between the real output, $o = f(net-θ)$, of the neuron and the target output, $t$, is minimized.\n",
        "\n",
        "- **Unsupervied learning** where the aim is to discover parrterns or features in the input data with no assistance from an external source. Many unsupervised learning algorithms basically perform a clustering of the training patterns.\n",
        "\n",
        "- **Reinforcement learning**: where the aim is to reward the neuron for good performance, and to penalize the neuron for bad performance.\n",
        "\n",
        "- **Augmented Vectors**: The input vector is augmented to include an additional input unit $z_{I+1}$, referred to as the $\\text{bias unit}$. The value of $z_{I+1}$ is always -1, and the weight $v_{I+1}$ serves as the value of the threshold. The net input signal to the $AN$ is then calculated:<br>\n",
        "$net = ∑_{i=1}^Iz_iv_i - θ$, where $θ = z_I + 1v_{I+1} = -v_{I+1}$\n",
        "\n",
        "In the case of the step function, an input vector yields an output of 1 when $∑_{i=1}^{I+1} z_iv_i ≥ 0$ and 0 when $∑_{i=1}^{I+1}z_iv_i < 0$.\n",
        "\n",
        "- **Gradient Descent Learning Rule**: GD requires the definition of an error function to measure the neuron's error in approximating the target. The sum of squared errors: $ε = ∑_{p=1}^{P_T}(t_p - O_p)^2$.\n",
        "\n",
        "  - $t_p$, $O_p$ are the target and actual output for the $p-th$ pattern, and $P_T$ is the total number of input-target vector pairs in the training set.\n",
        "\n",
        "  - The aim of GD is to find the weight values that minimize $ε$ in weight space\n",
        ", and to move the weight vector along the negative gradient.\n",
        "\n",
        "  - Given a single training pattern, weights are updated using:\n",
        "    $v_i(t) = v_i(t-1) + Δv_i(t)$\n",
        "\n",
        "    with $Δv_i(t) = ŋ(-\\frac{∂ε}{∂v_i})$\n",
        "\n",
        "    where $\\frac{∂ε}{∂v_i} = -2(t_p - O_p)\\frac{∂f}{∂net_p}z_{i,p}$\n",
        "\n",
        "    and $ŋ$ is the learning rate. The calculation of the partial derivative of $f$ with respect to $net_p$ presents a problem for all discontinuous activation functions, such as the step and ramp functions; $z_{i,p}$ is the $i-th$ input signal corresponding to pattern $p$.\n",
        "\n",
        "- **Widrow-Hoff Learning Rule**:\n",
        "  - $f = net_p$, then $\\frac{∂f}{∂net_p} = 1$.\n",
        "\n",
        "  - $\\frac{∂ε}{∂v_i} = -2(t_p - O_p)z_{i,p}$\n",
        "\n",
        "  - weights are then updated using:\n",
        "\n",
        "    $v_i(t) = v_i(t-1) + 2ŋ(t_p - O_p)z_{i,p}$\n",
        "\n",
        "    The WH learing rue, also referred to as the least-means-suqre (LSM) algorithm.\n",
        "\n",
        "- **Generalized Delta Learning Rule**: Assume that the sigmoid function is used, then:\n",
        "\n",
        "  $\\frac{∂f}{∂net_p} = O_p(1-O_p)$\n",
        "\n",
        "  giving:\n",
        "\n",
        "  $\\frac{∂ε}{∂v_i} = -2(t_p - O_p)O_p(1-O_p)z_{i,p}$\n",
        "\n",
        "- **Error-Correction Learning Rule**:When the binary-valued activation function are used (for expamle: the step function). Weights are only adjusted when the neuron responds in error. $(t_p - O_p) = 1 \\text{ or } (t_p - O_p) = - 1$."
      ],
      "metadata": {
        "id": "La-LE6IWT5kh"
      }
    }
  ]
}